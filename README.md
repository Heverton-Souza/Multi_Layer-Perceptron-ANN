# Multilayer Perceptron ANN

Este projeto implementa uma rede neural Perceptron de mÃºltiplas camadas (_Multilayer Perceptron - MLP_) para identificar dÃ­gitos cursivos 0 e 1. Os dÃ­gitos sÃ£o representados em uma matriz 4x4, resultando em 16 pixels binÃ¡rios como entrada para a rede.

## ğŸ“ Estrutura do RepositÃ³rio

- `mlp.py` - CÃ³digo principal do projeto, contendo o treinamento e reconhecimento dos dÃ­gitos.
- `pesos_oculta.txt` - Arquivo que armazena os pesos treinados da camada oculta.
- `pesos_saida.txt` - Arquivo que armazena os pesos treinados da camada de saÃ­da.
- `amostras.txt` - Arquivo contendo exemplos de entrada para treinamento e teste.

## ğŸš€ Como Executar

### 1ï¸âƒ£ Clonar o repositÃ³rio

```bash
$ git clone https://github.com/Heverton-Souza/Multi_Layer-Perceptron-ANN.git

$ cd Multilayer-Perceptron-ANN
```

### 2ï¸âƒ£ Instalar dependÃªncias

Este projeto utiliza a biblioteca `numpy`, que pode ser instalada com:

```bash
$ pip install numpy
```

### 3ï¸âƒ£ Executar o cÃ³digo

Para iniciar o treinamento ou reconhecer dÃ­gitos, basta rodar o script:

```bash
$ python mlp.py
```

O programa apresentarÃ¡ um menu:

1. **Treinar a rede neural** - Permite fornecer amostras e treinar a rede.
2. **Reconhecer um dÃ­gito** - Permite inserir uma nova amostra e testar a rede jÃ¡ treinada.
3. **Sair** - Finaliza o programa.

## ğŸ§  Como Funciona

### ğŸ”¹ Entrada de Dados

Cada amostra de treinamento ou teste Ã© uma sequÃªncia de 16 valores binÃ¡rios representando uma matriz 4x4. Exemplo de entrada:

```
0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 - Zero
0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 - Um
```

A rede recebe esses valores como entrada, adicionando um bias.

### ğŸ”¹ Estrutura do MLP

Diferente do _Single Layer Perceptron_, este MLP possui uma camada escondida com **1 neurÃ´nio**, antes da camada de saÃ­da. A estrutura Ã©:

- **Entrada**: 16 neurÃ´nios (+1 bias)
- **Camada escondida**: 1 neurÃ´nio
- **Camada de saÃ­da**: 2 neurÃ´nios (um para o dÃ­gito "0" e outro para "1")

### ğŸ”¹ FunÃ§Ã£o de AtivaÃ§Ã£o

Nesta rede, a ativaÃ§Ã£o das camadas usa a **funÃ§Ã£o sigmoide**, definida como:

```math
f(V) = \frac{1}{1 + e^{-a.V}}
```

onde `a` Ã© a taxa de aprendizado.

### ğŸ”¹ Treinamento

O treinamento ocorre ajustando os pesos por meio do **Backpropagation**, minimizando o erro. A condiÃ§Ã£o de parada Ã© diferente do SLP e baseada no erro quadrÃ¡tico mÃ©dio:

```math
E = \frac{1}{T} \sum_{i=1}^{T} E_i
```

onde `T` Ã© o nÃºmero total de amostras, e `Ei` Ã© o erro quadrÃ¡tico de cada amostra.

Se `E` for menor que um limite predefinido, o treinamento Ã© encerrado.

### ğŸ”¹ Reconhecimento

ApÃ³s o treinamento, novos dÃ­gitos podem ser classificados utilizando os pesos ajustados. O resultado indica se a amostra Ã© reconhecida como `0` ou `1`.

## ğŸ“Š Testes e Taxa de Acerto

O arquivo `amostras.txt` contÃ©m exemplos usados no treinamento e teste:

- **Primeiras 8 amostras**: usadas no treinamento.
- **Ãšltimas 8 amostras**: usadas para testar a rede apÃ³s o treinamento.

Nos testes realizados com as 8 amostras que nÃ£o foram usadas no treinamento, a rede acertou **8 de 8**, resultando em uma taxa de acerto de **100%**.

Este resultado Ã© significativamente superior ao do _Single Layer Perceptron_, alcanÃ§ado apenas com a adiÃ§Ã£o de **uma Ãºnica camada escondida com apenas um neurÃ´nio**.

## ğŸ“· Visualizando os DÃ­gitos

Para facilitar a compreensÃ£o, abaixo estÃ¡ um exemplo visual das representaÃ§Ãµes dos dÃ­gitos nas matrizes 4x4:

![Exemplo](assets/Exemplo.jpeg)
